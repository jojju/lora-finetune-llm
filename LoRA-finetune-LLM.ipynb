{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jojju/lora-finetune-llm/blob/main/LoRA-finetune-LLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Finetune a GPTQ quantized model using LoRA"
      ],
      "metadata": {
        "id": "H_D9kG_efts3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cY3RD1iKfKPD"
      },
      "outputs": [],
      "source": [
        "!pip install -q -U transformers peft accelerate optimum auto-gptq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load a quantized model"
      ],
      "metadata": {
        "id": "VVSMJL7uolks"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "\n",
        "model_id = \"TheBloke/openchat_3.5-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "yK0JlLm5gPix"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the model"
      ],
      "metadata": {
        "id": "64Q1kLX5g_Et"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "id": "bOigq7eFl6vq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inspect the quantization options"
      ],
      "metadata": {
        "id": "U7zYDr0AbHwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.config.quantization_config.to_dict()"
      ],
      "metadata": {
        "id": "Tg855O9EbFKI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate some text based on a line from Kipling's poem \"If\". The continuation of the poem should not be entirely correct."
      ],
      "metadata": {
        "id": "bd7Lwe1lLASl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = \"\"\"If you can dreamâ€”and not make dreams your master;\"\"\"\n",
        "\n",
        "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(0)\n",
        "out = model.generate(**inputs, max_new_tokens=60, do_sample=False)\n",
        "\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "bFYKL5RzmGI1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finetune the model using the PEFT library from Huggingface"
      ],
      "metadata": {
        "id": "l-EwnZd0jpwB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft.utils.other import prepare_model_for_kbit_training\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "Lio6QV7MgEeD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convert the model into a PEFT model"
      ],
      "metadata": {
        "id": "P5lRlaaEl0WK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from peft.tuners.lora import LoraConfig\n",
        "from peft.mapping import get_peft_model\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"k_proj\",\"o_proj\",\"q_proj\",\"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "BvbgQ8UclufV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get some simple training data from GitHub"
      ],
      "metadata": {
        "id": "Lg_IL0EuMyhI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "!git clone https://github.com/jojju/misc\n",
        "!mv ./misc/data/kipling_if ./misc/data/kipling_if.txt"
      ],
      "metadata": {
        "id": "CSNl2ohalLIF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Put the training data in a Dataset"
      ],
      "metadata": {
        "id": "3sEQ0nW0m9I4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset, Dataset, DatasetDict\n",
        "\n",
        "with open('./misc/data/kipling_if.txt', 'r', encoding='utf-8') as file:\n",
        "   poem = file.read()\n",
        "\n",
        "data_dict = {\"text\": [poem]}\n",
        "dataset = Dataset.from_dict(data_dict)\n",
        "data = DatasetDict()\n",
        "data[\"train\"] = dataset\n",
        "\n",
        "data = data.map(lambda x: tokenizer(x[\"text\"]))\n",
        "\n",
        "# Print the training data text\n",
        "print(data[\"train\"][0][\"text\"])"
      ],
      "metadata": {
        "id": "QOLRaTq2ox90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Perform the finetuning"
      ],
      "metadata": {
        "id": "scrFpCj7N40O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, DataCollatorForLanguageModeling\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    train_dataset=data[\"train\"][\"input_ids\"],\n",
        "    args=TrainingArguments(\n",
        "        per_device_train_batch_size=1,\n",
        "        warmup_steps=2,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=\"outputs\",\n",
        "        optim=\"adamw_torch\",\n",
        "        num_train_epochs=5,\n",
        "        include_tokens_per_second=True\n",
        "    ),\n",
        "    data_collator=DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
        ")\n",
        "\n",
        "# Print the training tokens\n",
        "print(trainer.train_dataset)\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "vT0XjNc2jYKy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Save the finetuning to disk"
      ],
      "metadata": {
        "id": "8iJdzTtWObXi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = \"saved_models\"\n",
        "trainer.save_model(save_dir)\n"
      ],
      "metadata": {
        "id": "p2AgkYW6euMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -l saved_models"
      ],
      "metadata": {
        "id": "yak5VFkolr_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load the finetuning from disk"
      ],
      "metadata": {
        "id": "tBYyVL3EOvml"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_base = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)"
      ],
      "metadata": {
        "id": "tZ2vPqFHs3FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft.peft_model import PeftModel\n",
        "\n",
        "model_with_lora = PeftModel.from_pretrained(model_base, save_dir)"
      ],
      "metadata": {
        "id": "gkMA0F86fYHk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use the finetuned model to generate text based on the same prompt as above. Note that the output is now (hopefully) correct."
      ],
      "metadata": {
        "id": "0AE_5IJJPDbY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = tokenizer(sample_text, return_tensors=\"pt\").to(0)\n",
        "out = model_with_lora.generate(**inputs, max_new_tokens=62, do_sample=False)\n",
        "\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "drtyaztrPyeu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}